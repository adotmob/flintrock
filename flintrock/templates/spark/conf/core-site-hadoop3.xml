<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>{hadoop_ephemeral_dirs}</value>
  </property>

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://{master_ip}:9000</value>
    <description> By default, write to HDFS if no scheme is specified</description>
  </property>

  <property>
    <name>fs.s3a.endpoint</name>
    <value>s3-{region}.amazonaws.com</value>
    <final>true</final>
    <description>AWS S3 endpoint to connect to. An up-to-date list is
      provided in the AWS Documentation: regions and endpoints. Without this
      property, the standard region (s3.amazonaws.com) is assumed.
    </description>
  </property>

  <property>
    <name>fs.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
  </property>

  <property>
    <name>fs.s3a.buffer.dir</name>
    <value>/home/ec2-user/spark/work,/tmp</value>
  </property>

  <property>
    <name>fs.s3a.fast.upload</name>
    <value>true</value>
    <description> NEW in hadoop 2.7 and UNSTABLE.
      Upload directly from memory instead of buffering to
      disk first. Memory usage and parallelism can be controlled as up to
      fs.s3a.multipart.size memory is consumed for each (part)upload actively
      uploading (fs.s3a.threads.max) or queueing (fs.s3a.max.total.tasks)
    </description>
  </property>

  <property>
    <name>fs.s3a.committer.name</name>
    <value>magic</value>
    <description>
      Committer to create for output to S3A, one of:
      "file", "directory", "partitioned", "magic".
    </description>
  </property>

  <property>
    <name>fs.s3a.committer.magic.enabled</name>
    <value>true</value>
    <description>
      Enable support in the filesystem for the S3 "Magic" committer.
      When working with AWS S3, S3Guard must be enabled for the destination
      bucket, as consistent metadata listings are required.
    </description>
  </property>

  <property>
    <name>fs.s3a.bucket.all.committer.magic.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>fs.s3a.attempts.maximum</name>
    <value>50</value>
    <description>How many times we should retry commands on transient errors.</description>
  </property>

  <property>
    <name>fs.s3a.connection.maximum</name>
    <value>1000</value>
    <description>Controls the maximum number of simultaneous connections to S3.
      NB: fs.s3a.connection.maximum must be larger than fs.s3a.threads.max,
      to avoid org.apache.http.conn.ConnectionPoolTimeoutException
    </description>
  </property>

  <property>
    <name>google.cloud.auth.service.account.enable</name>
    <value>true</value>
  </property>

  <property>
    <name>google.cloud.auth.service.account.json.keyfile</name>
    <value>/home/ec2-user/google-service-account.json</value>
    <description>
      The JSON key file of the service account used for GCS
      access when google.cloud.auth.service.account.enable is true.
    </description>
  </property>
</configuration>